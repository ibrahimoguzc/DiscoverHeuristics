import json
import time
import os
import multiprocessing
from typing import Collection, Any

import requests
from implementation import funsearch
from implementation import config
from implementation import sampler
from implementation import evaluator_accelerate
from implementation import evaluator
from implementation import code_manipulation
import bin_packing_utils
import pandas as pd






# os.environ['CUDA_VISIBLE_DEVICES'] = "0"


def _trim_preface_of_body(sample: str) -> str:
    """Trim the redundant descriptions/symbols/'def' declaration before the function body.
    Please see my comments in sampler.LLM (in sampler.py).
    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.

    -Example sample (function & description generated by LLM):
    -------------------------------------
    This is the optimized function ...
    def priority_v2(...) -> ...:
        return ...
    This function aims to ...
    -------------------------------------
    -This function removes the description above the function's signature, and the function's signature.
    -The indent of the code is preserved.
    -Return of this function:
    -------------------------------------
        return ...
    This function aims to ...
    -------------------------------------
    """
    lines = sample.splitlines()
    func_body_lineno = 0
    find_def_declaration = False
    for lineno, line in enumerate(lines):
        # find the first 'def' statement in the given code
        if line[:3] == 'def':
            func_body_lineno = lineno
            find_def_declaration = True
            break
    if find_def_declaration:
        code = ''
        indent = '    '
        for line in lines[func_body_lineno + 1:]:
            if line[:4] != indent:
                line = indent + line
            code += line + '\n'
        return code
    return sample


class LocalLLM(sampler.LLM):
    """Language model that predicts continuation of provided source code.
    """

    def __init__(self, samples_per_prompt: int, batch_inference: bool = True, trim=True) -> None:
        """
        Args:
            batch_inference: Use batch inference when sample functions. The batch size equals to the samples_per_prompt.
        """
        super().__init__(samples_per_prompt)
        url = 'http://127.0.0.1:3000/completions'
        # additional_prompt = ('Complete a different and more complex Python function. '
        #                      'Be creative and you can insert multiple if-else and for-loop in the code logic.'
        #                      'Only output the Python code, no descriptions.')
        # additional_prompt = ('Can you give me the complete version of last function? Only output the code of completed function, no description.')
        # additional_prompt = ("Can you give me the complete version of last function?\nUse your knowledge about physical relations of inputs.\nOnly output the Python code, no description.\nAim for concise code.\n\n")
        # additional_prompt = ("Give me the complete version of last function.\nConsider the physical relationships of the inputs while creating the function and return the output.\nAim for code that is concise yet clear.")
        # Return output at the end of function.\nAim for code that is concise yet clear.\n
        
        ss_instruct = '\nInstead of if-else conditions, use smooth functions.'
        additional_prompt = ("Can you give me the complete version of last function?\nConsider physical relationships of inputs while completing the function."+"\nAim for concise code.\n\n")
        # additional_prompt = ("Can you give me the complete version of last function?\nConsider physical relationships of inputs while completing the function.\n Give the second state equation of a custom nonlinear damped oscillator (dv/dt) dynamical system given x , v ."+"\nOnly output the Python code, no description.\nAim for concise code.\n\n")
        

        #Job Scheduling
        # additional_prompt = ("Can you give me the complete version of last function?\nConsider your prior knowledge about problem while completing the function.\n Aim for concise code.\n\n")
        
        
        self._batch_inference = batch_inference
        self._url = url
        self._additional_prompt = additional_prompt
        self._trim = trim

    def draw_samples(self, prompt: str) -> Collection[str]:
        """Returns multiple predicted continuations of `prompt`.
        """
        ############## FOR INSTRUCT MODELS ##################
        prompt = '\n'.join([self._additional_prompt, prompt])
        #############################################################
        while True:
            try:
                all_samples = []
                if self._batch_inference:
                    response = self._do_request(prompt)
                    for res in response:
                        all_samples.append(res)
                else:
                    for _ in range(self._samples_per_prompt):
                        response = self._do_request(prompt)
                        all_samples.append(response)

                # breakpoint()
                # trim samples
                if self._trim:
                    all_samples = [_trim_preface_of_body(sample) for sample in all_samples]

                # breakpoint()
                return all_samples
            except:
                continue

    def _do_request(self, content: str) -> str:
        content = content.strip('\n').strip()
        # repeat the prompt for batch inference (inorder to decease the sample delay)
        repeat_prompt: int = self._samples_per_prompt if self._batch_inference else 1
        data = {
            'prompt': content,
            'repeat_prompt': repeat_prompt,
            'params': {
                'do_sample': True,
                'temperature': None,
                'top_k': None,
                'top_p': None,
                'add_special_tokens': False,
                'skip_special_tokens': True,
            }
        }
        headers = {'Content-Type': 'application/json'}
        response = requests.post(self._url, data=json.dumps(data), headers=headers)
        if response.status_code == 200:
            response = response.json()["content"]
            return response if self._batch_inference else response[0]


class Sandbox(evaluator.Sandbox):
    """Sandbox for executing generated code. Implemented by RZ.

    RZ: Sandbox returns the 'score' of the program and:
    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).
    2) stops the execution of the code in time (avoid endless loop).
    """

    def __init__(self, verbose=False, numba_accelerate=False):
        """
        Args:
            verbose         : Print evaluate information.
            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions
                              support numba acceleration, such as np.piecewise().
        """
        self._verbose = verbose
        self._numba_accelerate = numba_accelerate

    def run(
            self,
            program: str,
            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')
            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.
            inputs: Any,  # refers to the dataset
            test_input: str,  # refers to the current instance
            timeout_seconds: int,
            **kwargs  # RZ: add this
    ) -> tuple[Any, bool]:
        """Returns `function_to_run(test_input)` and whether execution succeeded.

        RZ: If the generated code (generated by LLM) is executed successfully,
        the output of this function is the score of a given program.
        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.
        """
        dataset = inputs[test_input]
        result_queue = multiprocessing.Queue()
        
        #ADDED FOR DEBUGGING
        # ali = self._compile_and_run_function(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)
        # breakpoint()
        
        
        process = multiprocessing.Process(
            target=self._compile_and_run_function,
            args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)
        )
        process.start()
        process.join(timeout=timeout_seconds)
        if process.is_alive():
            # if the process is not finished in time, we consider the program illegal
            process.terminate()
            process.join()
            results = None, False
        else:
            # if not result_queue.empty():
            #     results = result_queue.get_nowait()
            for _ in range(5):  # Check 5 times with 0.1 second interval
                if not result_queue.empty():
                    results = result_queue.get_nowait()
                    break
                time.sleep(0.1)
            else:
                results = None, False
        # breakpoint()
        
        if self._verbose:
            print(f'================= Evaluated Program =================')
            program_: code_manipulation.Program = code_manipulation.text_to_program(text=program)
            func_to_evolve_: str = kwargs.get('func_to_evolve', 'priority')
            function_: code_manipulation.Function = program_.get_function(func_to_evolve_)
            function_: str = str(function_).strip('\n')
            print(f'{function_}')
            print(f'-----------------------------------------------------')
            print(f'Score: {str(results)}')
            print(f'=====================================================')
            print(f'\n\n')

        return results

    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,
                                  result_queue):
        try:
            # optimize the code (decorate function_to_run with @numba.jit())
            if numba_accelerate:
                program = evaluator_accelerate.add_numba_decorator(
                    program=program,
                    function_to_evolve=function_to_evolve
                )
            # compile the program, and maps the global func/var/class name to its address
            all_globals_namespace = {}
            # execute the program, map func/var/class to global namespace
            exec(program, all_globals_namespace)
            # get the pointer of 'function_to_run'
            function_to_run = all_globals_namespace[function_to_run]
            # return the execution results
            results = function_to_run(dataset)
            # breakpoint()
            # the results must be int or float
            if not isinstance(results, (int, float)):
                result_queue.put((None, False))
                return
            result_queue.put((results, True))
        except Exception as e:
            # if raise any exception, we assume the execution failed
            print(f"Exception occurred-Execution Error: {e}")
            result_queue.put((None, False))



with open(
        # os.path.join("specification_equation_discovery_1d.txt"),
        # os.path.join("specification_feyn_I_12_2_bfgs.txt"),
        # os.path.join("specification_feyn_test_1_bfgs.txt"),
        # os.path.join("specification_haigenpoiseulle_bfgs.txt"),
        # os.path.join("specification_michaelesmantes_bfgs.txt"),
        # os.path.join("specification_rosenthal_bfgs.txt"),
        # os.path.join("specification_oscillator_bfgs.txt"),
        # os.path.join("specification_oscillator2_bfgs.txt"),
        # os.path.join("specification_bactgrow_bfgs.txt"),
        # os.path.join("specification_stressstrain_bfgs.txt"),
        # os.path.join("specification_equation_discovery_1d._torchopt.txt"),
        os.path.join("specification_1d_jobscheduling.txt"),
        encoding="utf-8",
    ) as f:
        specification = f.read()

# It should be noted that the if __name__ == '__main__' is required.
# Because the inner code uses multiprocess evaluation.
if __name__ == '__main__':
    class_config = config.ClassConfig(llm_class=LocalLLM, sandbox_class=Sandbox)
    config = config.Config(samples_per_prompt=4, evaluate_timeout_seconds=30)

    global_max_sample_num = 10000  # if it is set to None, funsearch will execute an endless loop
    #LOAD SPECIFICATION
    with open(
        # os.path.join("specification_equation_discovery_1d.txt"),
        # os.path.join("specification_feyn_I_12_2_bfgs.txt"),
        # os.path.join("specification_feyn_test_1_bfgs.txt"),
        # os.path.join("specification_haigenpoiseulle_bfgs.txt"),
        # os.path.join("specification_michaelesmantes_bfgs.txt"),
        # os.path.join("specification_rosenthal_bfgs.txt"),
        # os.path.join("specification_oscillator_bfgs.txt"),
        # os.path.join("specification_oscillator2_bfgs.txt"),
        # os.path.join("specification_bactgrow_bfgs.txt"),
        # os.path.join("specification_stressstrain_bfgs.txt"),
        # os.path.join("specification_equation_discovery_1d._torchopt.txt"),
        os.path.join("specification_1d_jobscheduling.txt"),
        encoding="utf-8",
    ) as f:
        specification = f.read()
    
    #dataset
    import numpy as np
    import torch
    #Lod PMLB Datasets
    # X, y = load_pmlb('feynman','feynman_I_12_2')
    # X, y = load_pmlb('feynman','feynman_test_1')
    
    #Load Other Datasets
    # eq_name = 'haigen-poiseulle'
    # eq_name = 'michaeles-mantes'
    # eq_name = 'rosenthal'
    # eq_name = 'bactgrow2'
    # eq_name = 'bactgrow3_12'
    # eq_name = 'oscillator1'
    # eq_name = 'oscillator2'
    # eq_name = 'oscillator3'
    eq_name = 'm1jobs'
    # X_train = np.load('./data/'+eq_name+'/X_train.npy')[:]
    # y_train = np.load('./data/'+eq_name+'/y_train.npy')[:]
    # X_val = np.load('./data/'+eq_name+'/X_val.npy')[:]
    # y_val = np.load('./data/'+eq_name+'/y_val.npy')[:]
    data_job1m = np.load('data/data_25_10000_immutable.npy', allow_pickle=True)

    
    # eq_name = 'stress-strain2'
    # df = pd.read_csv('./data/'+eq_name+'/Al_stress_norm.csv')
    # data = np.array(df)
    # X = data[:, :-1]
    # y = data[:, -1].reshape(-1)
    # breakpoint()
    # data_dict = {'inputs_train': X_train, 'outputs_train': y_train, 'inputs_val': X_val, 'outputs_val':y_val}
    # data_dict = {'inputs': X, 'outputs': y}
    # dataset = {'ReLU': data_dict}

    dataset = {'ReLU': data_job1m}
    ##################################################
    
    
    funsearch.main(
        specification=specification,
        inputs=dataset,
        config=config,
        max_sample_nums=global_max_sample_num,
        class_config=class_config,
        # log_dir='logs/prompt-design/michaeles-mantes_mixtral',
        # log_dir='logs/prompt-design/rosenthal_mixtral-v8',
        # log_dir='logs/prompt-design/rosenthal_starcoder',
        # log_dir='logs/prompt-design/haigen-poiseulle_starcoder',
        # log_dir='logs/prompt-design/michaeles-mantes_starcoder',
        # log_dir='logs/prompt-design/bactgrow_mixtral-v8',
        # log_dir = 'logs/prompt-design/bactgrow322_mixtral-v1',
        # log_dir = 'logs/bactgrow312_mixtral-v3',
        # log_dir='logs/prompt-design/stressstrain_mixtral-v3',
        # log_dir='logs/prompt-design/oscillator1_mixtral-v4',
        # log_dir = 'logs/oscillator2_mixtral-v2-3',
        # log_dir = 'logs/oscillator3_mixtral-v2-2',
        # log_dir = 'logs/m1jobs-mixtral-v10',
        # log_dir='logs/feynman-I-12-2_mixtral',
        log_dir='logs/smsmt_immutable',
    )
